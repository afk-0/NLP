{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7205af0",
   "metadata": {},
   "source": [
    "# Removing punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93abfd1a",
   "metadata": {},
   "source": [
    "## why remove punctuation ?\n",
    "\n",
    "The main reason to separate punctuation by spacing in a sentence is to get a vector representation of words. \n",
    "\n",
    "BERT can handle punctuation, smileys etc. Of course, smileys contribute a lot to sentiment analysis. So, don't remove them.\n",
    "\n",
    "#### Does punctuation affect BERT?\n",
    "Our findings can be broadly summarized as follows: (1) irrelevant changes in punctuation are correctly ignored by the recent transformer models (BERT) while older RNN-based mod- els were sensitive to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4010423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6436c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"string. with. Punctuation?\"\n",
    "s = re.sub(r'[^\\w\\s]','', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b35c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string with Punctuation\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb24993",
   "metadata": {},
   "source": [
    "# Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21f2456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9afcdfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eighty', 'seven', 'miles', 'to', 'go', 'yet', 'Onward']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d49df6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Think', 'and', 'wonder', 'wonder', 'and', 'think']\n"
     ]
    }
   ],
   "source": [
    "sentence  = \"Think and wonder, wonder and think.\"\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "new_words = tokenizer.tokenize(sentence)\n",
    "\n",
    "print(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0728e60e",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "Stop words are a set of commonly used words in any language. For example, in English, “the”, “is” and “and”, would easily qualify as stop words.\n",
    "\n",
    "In NLP and text mining applications, stop words are used to eliminate unimportant words, allowing applications to focus on the important words instead.\n",
    "\n",
    "\n",
    "# Why remove stopwords?\n",
    "Stop words are often removed from the text before training deep learning and machine learning models since stop words occur in abundance, hence providing little to no unique information that can be used for classification or clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476ba99",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f555a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4148d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eba0b9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nick', 'likes', 'play', 'football', ',', 'fond', 'tennis', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "text_tokens = word_tokenize(text)\n",
    "\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536f4a28",
   "metadata": {},
   "source": [
    "# SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a6dfa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nick', 'likes', 'play', 'football', ',', 'not', 'fond', 'tennis', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "all_stopwords = sp.Defaults.stop_words\n",
    "all_stopwords.remove('not')\n",
    "\n",
    "text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "text_tokens = word_tokenize(text)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e645a9",
   "metadata": {},
   "source": [
    "# using custom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b6edfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d65ef8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mystopwords(sentence):\n",
    "    tokens = sentence.split(\" \")\n",
    "    tokens_filtered= [word for word in text_tokens if not word in my_stopwords]\n",
    "    return (\" \").join(tokens_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "504e012d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nick likes play football , however fond tennis .\n"
     ]
    }
   ],
   "source": [
    "text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "filtered_text = remove_mystopwords(text)\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca2f671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfd028b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4570c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b6a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6c223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1796156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
