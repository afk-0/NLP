{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480f7043",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f855ff09",
   "metadata": {},
   "source": [
    "#### Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. Tokenization can be done to either separate words or sentences. If the text is split into words using some separation technique it is called word tokenization and same separation done for sentences is called sentence tokenization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb98bc",
   "metadata": {},
   "source": [
    "## why use it ? \n",
    "#### This process is important because the meaning of the text can be interpreted through analysis of the words present in the text. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70361a4c",
   "metadata": {},
   "source": [
    "#### The basic thing about tokenization, is spliting the word, but why dont use the conventional split method. The main issue with normal conventional  is that, you cant differntiate between a full stop. For example : Dr. and sentence end. Normal conventional method cant do that, that is why use methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea93ece",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d7a8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72d5c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de934834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this -> sentence -> will -> turn -> into -> tokens -> now -> , -> Dr. -> "
     ]
    }
   ],
   "source": [
    "doc = nlp(\"this sentence will turn into tokens now, Dr.\")\n",
    "for token in doc:\n",
    "    print(token, end= \" -> \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "373422aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let -> 's -> go -> to -> N.Y -> ! -> "
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"let's go to N.Y!\")\n",
    "for token in doc1:\n",
    "    print(token, end= \" -> \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a80c2eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence will turn into"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f33e73f",
   "metadata": {},
   "source": [
    "### Operations on token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78fbbc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = doc[0]\n",
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ae87869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "this"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e73e5f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cb8358d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "536a7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"$\"\n",
    "doc = nlp(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f30e09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2 = doc[0]\n",
    "token2.is_currency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e7118",
   "metadata": {},
   "source": [
    "## Example : suppose you have data of your students and you want to find mail of your student to send them  an important announuncement. how you can use something ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d60a2790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"student.txt\") as f :\n",
    "    text = f.readlines()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4bff3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n \\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4ef1fa",
   "metadata": {},
   "source": [
    "## how you can get email ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71c672cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virat@kohli.com\n",
      "maria@sharapova.com\n",
      "serena@williams.com\n",
      "joe@root.com\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for token in doc: \n",
    "    if token.like_email: # you can use multiple methods like this to know more about your type.\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffab89a",
   "metadata": {},
   "source": [
    "## customizing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26d8ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1de7a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.tokenizer.add_special_case(\"gimme\",\n",
    "                               [\n",
    "                                   {ORTH:\"gim\"},\n",
    "                                   {ORTH:\"me\"}\n",
    "                               ])\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c2fc6",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8566ea45",
   "metadata": {},
   "source": [
    "###### This book has references to many websites from where you can download free datasets. You are an NLP engineer working for some company and you want to collect all dataset websites from this book. To keep exercise simple you are given a paragraph from this book and you want to grab all urls from this paragraph using spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05ec6c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fbb25deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25d3f473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.data.gov/\n",
      "http://www.science\n",
      "http://data.gov.uk/.\n",
      "http://www3.norc.org/gss+website/\n",
      "http://www.europeansocialsurvey.org/.\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.like_url:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782aed8f",
   "metadata": {},
   "source": [
    "##### (2) Extract all money transaction from below sentence along with currency. Output should be,\n",
    "\n",
    "two $\n",
    "\n",
    "500 €"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9acf2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "transaction = nlp(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "33da2aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "for token in transaction:\n",
    "    if token.is_currency:\n",
    "        print(transaction[token.i-1], token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab608b",
   "metadata": {},
   "source": [
    "# nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "953e8005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d744fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c442666",
   "metadata": {},
   "outputs": [],
   "source": [
    " s = '''Good muffins cost $3.88\\nin New York.  Please buy me\n",
    " two of them.\\n\\nThanks.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a28dac2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af3520bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3',\n",
       " '.',\n",
       " '88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f269f7",
   "metadata": {},
   "source": [
    "### wordpunct_tokenize is based on a simple regexp tokenization. Basically it uses the regular expression \\w+|[^\\w\\s]+ to split the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d447bab",
   "metadata": {},
   "source": [
    "### word_tokenize on the other hand is based on a TreebankWordTokenizer. It basically tokenizes text like in the Penn Treebank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49286bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ec3b6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good muffins cost $3.88\\nin New York.',\n",
       " 'Please buy me\\ntwo of them.',\n",
       " 'Thanks.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b43fc846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.'],\n",
       " ['Please', 'buy', 'me', 'two', 'of', 'them', '.'],\n",
       " ['Thanks', '.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_tokenize(t) for t in sent_tokenize(s)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
