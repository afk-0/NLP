{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9050006",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab57b4b",
   "metadata": {},
   "source": [
    "we find the word stem, which is the base word. history -> histori <- historical. It converts the word into its base word. base word also calls lemma. \n",
    "\n",
    "## why use it ?\n",
    "Stemming is a process where words are reduced to a root by removing inflection through dropping unnecessary characters, usually a suffix.Stemming and AI knowledge extract meaningful information from vast sources like big data or the Internet since additional forms of a word related to a subject may need to be searched to get the best results. \n",
    "#### Like changing, changed , change -> after performing stemming -> stem\n",
    "sometimes it doesn't make any sense. \n",
    "\n",
    "## when to use it ? \n",
    "go with stemming when the vocab space is small and the documents are large. Conversely, go with word embeddings when the vocab space is large but the documents are small. postive, negative sentiment analysis, gmail classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0740d351",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "Lemmatization is a bit more complex in that the computer can group together words that do not have the same stem, but still have the same inflected meaning.\n",
    "\n",
    "#### incase of stemming the word doesn't have any meaning, where as lemmatization do have some meaning. Lemmatization takes more time as its word have meaning, where as stemming dont take as much time as lemmatization.  \n",
    "\n",
    "#### lemmatization provides better results by performing an analysis that depends on the word's part-of-speech and producing real, dictionary words. As a result, lemmatization is harder to implement and slower compared to stemming\n",
    "\n",
    "#### history -> history <- historical , (finally, final, finalized) -> final\n",
    "\n",
    "\n",
    "# why and where to use ?\n",
    "\n",
    "chatbots, question answering . Because here you have to understand what its saying and get a meaning of it. so that is why we cant use stemming here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af17773",
   "metadata": {},
   "source": [
    "spacy dont support stemming , nltk has both stemming and lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "95c0deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8576d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3c56adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words= [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e470b19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating   -> after stemming  ->  eat\n",
      "eats   -> after stemming  ->  eat\n",
      "eat   -> after stemming  ->  eat\n",
      "ate   -> after stemming  ->  ate\n",
      "adjustable   -> after stemming  ->  adjust\n",
      "rafting   -> after stemming  ->  raft\n",
      "ability   -> after stemming  ->  abil\n",
      "meeting   -> after stemming  ->  meet\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word, \"  -> after stemming  -> \", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa191a",
   "metadata": {},
   "source": [
    "#### above you can see when perfoming ability -> it transformed into abil, which basically makes no sense.  that is why people preffer stemming over lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4331e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4a0641c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"eating eats eat ate adjustable rafting ability meeting better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ca26252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eating\n",
      "eats  |  eat\n",
      "eat  |  eat\n",
      "ate  |  eat\n",
      "adjustable  |  adjustable\n",
      "rafting  |  raft\n",
      "ability  |  ability\n",
      "meeting  |  meeting\n",
      "better  |  well\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c9b5a0",
   "metadata": {},
   "source": [
    "#### our model sometimes dont understand slang words like, bruh and bro is same, so they dont change them at all, at this time we can customize our model based on our need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0239d50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ccd8f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro | Brother\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "Brah | Brother\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      "! | !\n",
      "I | I\n",
      "am | be\n",
      "exhausted | exhaust\n"
     ]
    }
   ],
   "source": [
    "ar = nlp.get_pipe('attribute_ruler')\n",
    "\n",
    "ar.add([[{\"TEXT\":\"Bro\"}],[{\"TEXT\":\"Brah\"}]],{\"LEMMA\":\"Brother\"})\n",
    "\n",
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
    "for token in doc:\n",
    "    print(token.text, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0c0706",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8cc5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_words = ['running', 'painting', 'walking', 'dressing', 'likely', 'children', 'whom', 'good', 'ate', 'fishing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea51c5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running   ->   run\n",
      "painting   ->   paint\n",
      "walking   ->   walk\n",
      "dressing   ->   dress\n",
      "likely   ->   like\n",
      "children   ->   children\n",
      "whom   ->   whom\n",
      "good   ->   good\n",
      "ate   ->   ate\n",
      "fishing   ->   fish\n"
     ]
    }
   ],
   "source": [
    "#using stemming in nltk\n",
    "for word in lst_words:\n",
    "    print(word, \"  ->  \", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bdeda156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using lemmatization in spacy\n",
    "doc = nlp(\"running painting walking dressing likely children who good ate fishing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "31485bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running  |  run\n",
      "painting  |  painting\n",
      "walking  |  walking\n",
      "dressing  |  dress\n",
      "likely  |  likely\n",
      "children  |  child\n",
      "who  |  who\n",
      "good  |  good\n",
      "ate  |  eat\n",
      "fishing  |  fishing\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f87e5",
   "metadata": {},
   "source": [
    "# Exercise2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072d6614",
   "metadata": {},
   "source": [
    "convert the given text into it's base form using both stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b7c93d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp1 = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "88b6eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Latha is very multi talented girl.She is good at many skills like dancing, running, singing, playing.She also likes eating Pav Bhagi. she has a \n",
    "habit of fishing and swimming too.Besides all this, she is a wonderful at cooking too.\n",
    "\"\"\"\n",
    "doc = nlp1(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "59eb0ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latha   ->   latha\n",
      "is   ->   is\n",
      "very   ->   veri\n",
      "multi   ->   multi\n",
      "talented   ->   talent\n",
      "girl   ->   girl\n",
      ".   ->   .\n",
      "She   ->   she\n",
      "is   ->   is\n",
      "good   ->   good\n",
      "at   ->   at\n",
      "many   ->   mani\n",
      "skills   ->   skill\n",
      "like   ->   like\n",
      "dancing   ->   danc\n",
      ",   ->   ,\n",
      "running   ->   run\n",
      ",   ->   ,\n",
      "singing   ->   sing\n",
      ",   ->   ,\n",
      "playing   ->   play\n",
      ".   ->   .\n",
      "She   ->   she\n",
      "also   ->   also\n",
      "likes   ->   like\n",
      "eating   ->   eat\n",
      "Pav   ->   pav\n",
      "Bhagi   ->   bhagi\n",
      ".   ->   .\n",
      "she   ->   she\n",
      "has   ->   ha\n",
      "a   ->   a\n",
      "\n",
      "   ->   \n",
      "\n",
      "habit   ->   habit\n",
      "of   ->   of\n",
      "fishing   ->   fish\n",
      "and   ->   and\n",
      "swimming   ->   swim\n",
      "too   ->   too\n",
      ".   ->   .\n",
      "Besides   ->   besid\n",
      "all   ->   all\n",
      "this   ->   thi\n",
      ",   ->   ,\n",
      "she   ->   she\n",
      "is   ->   is\n",
      "a   ->   a\n",
      "wonderful   ->   wonder\n",
      "at   ->   at\n",
      "cooking   ->   cook\n",
      "too   ->   too\n",
      ".   ->   .\n",
      "\n",
      "   ->   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using stemming in nltk\n",
    "\n",
    "#step1: Word tokenizing\n",
    "list_ = []\n",
    "for token in doc:\n",
    "    list_.append(str(token))\n",
    "\n",
    "for word in list_:\n",
    "    print(word, \"  ->  \", stemmer.stem(word))\n",
    "#step2: getting the base form for each token using stemmer\n",
    "\n",
    "\n",
    "\n",
    "#step3: joining all words in a list into string using 'join()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c53e40c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatzation \n",
    "doc3 = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4cd87ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latha  |  Latha\n",
      "is  |  be\n",
      "very  |  very\n",
      "multi  |  multi\n",
      "talented  |  talented\n",
      "girl  |  girl\n",
      ".  |  .\n",
      "She  |  she\n",
      "is  |  be\n",
      "good  |  good\n",
      "at  |  at\n",
      "many  |  many\n",
      "skills  |  skill\n",
      "like  |  like\n",
      "dancing  |  dancing\n",
      ",  |  ,\n",
      "running  |  running\n",
      ",  |  ,\n",
      "singing  |  singing\n",
      ",  |  ,\n",
      "playing  |  play\n",
      ".  |  .\n",
      "She  |  she\n",
      "also  |  also\n",
      "likes  |  like\n",
      "eating  |  eat\n",
      "Pav  |  Pav\n",
      "Bhagi  |  Bhagi\n",
      ".  |  .\n",
      "she  |  she\n",
      "has  |  have\n",
      "a  |  a\n",
      "\n",
      "  |  \n",
      "\n",
      "habit  |  habit\n",
      "of  |  of\n",
      "fishing  |  fishing\n",
      "and  |  and\n",
      "swimming  |  swim\n",
      "too  |  too\n",
      ".  |  .\n",
      "Besides  |  besides\n",
      "all  |  all\n",
      "this  |  this\n",
      ",  |  ,\n",
      "she  |  she\n",
      "is  |  be\n",
      "a  |  a\n",
      "wonderful  |  wonderful\n",
      "at  |  at\n",
      "cooking  |  cook\n",
      "too  |  too\n",
      ".  |  .\n",
      "\n",
      "  |  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc3:\n",
    "    print(token, \" | \", token.lemma_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
